
<div align="center">

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•
```

# PriceCom â€” Real-time E-commerce Intelligence Engine
### Cross-Platform Price Intelligence Â· Semantic Product Matching Â· Blockchain-Grade Data Integrity

[![Django](https://img.shields.io/badge/Django-4.2+-092E20?style=for-the-badge&logo=django&logoColor=white)]()
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)]()
[![MySQL](https://img.shields.io/badge/Database-MySQL-4479A1?style=for-the-badge&logo=mysql&logoColor=white)]()
[![Selenium](https://img.shields.io/badge/Engine-Selenium-43B02A?style=for-the-badge&logo=selenium&logoColor=white)]()
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen?style=for-the-badge)]()

---

*Not a scraper. A living, self-updating intelligence layer that bridges Amazon and Flipkart  
into a single, cryptographically-verified price matrix.*

</div>

---

## TABLE OF CONTENTS

1. [Executive Overview & Value Proposition](#1-executive-overview--value-proposition)
2. [Architectural Deep-Dive (MVT + Background Pipeline)](#2-architectural-deep-dive-mvt--background-pipeline)
3. [Module Deep-Dive â€” The Intelligence Layer](#3-module-deep-dive--the-intelligence-layer)
   - 3.1 [Fuzzy Matchmaker â€” Semantic Product Engine](#31-fuzzy-matchmaker--semantic-product-engine)
   - 3.2 [Atomic Price Sync â€” Self-Updating Data Model](#32-atomic-price-sync--self-updating-data-model)
   - 3.3 [Freshness Heartbeat â€” UTC-Aware Data Pulse](#33-freshness-heartbeat--utc-aware-data-pulse)
   - 3.4 [Predictive Price Intelligence â€” LSTM/Prophet Ready](#34-predictive-price-intelligence--lstmprophet-ready)
   - 3.5 [Wallet & Reward Bridge â€” Loyalty Extrapolation](#35-wallet--reward-bridge--loyalty-extrapolation)
   - 3.6 [Audit Trail â€” Non-Repudiable Notification Engine](#36-audit-trail--non-repudiable-notification-engine)
   - 3.7 [OCR Price Extraction â€” Tesseract Vision Engine](#37-ocr-price-extraction--tesseract-vision-engine)
4. [Cybersecurity & Data Integrity â€” The Shield](#4-cybersecurity--data-integrity--the-shield)
   - 4.1 [SSRF Shield â€” URL Sanitization](#41-ssrf-shield--url-sanitization)
   - 4.2 [SHA-256 Financial Integrity Hashing](#42-sha-256-financial-integrity-hashing)
   - 4.3 [Stealth Scraping Engine](#43-stealth-scraping-engine)
5. [Deployment & Operation Guide](#5-deployment--operation-guide)
   - 5.1 [Prerequisites](#51-prerequisites)
   - 5.2 [Installation](#52-installation)
   - 5.3 [Redis & Celery Background Pipeline](#53-redis--celery-background-pipeline)
   - 5.4 [Environment Configuration](#54-environment-configuration)
6. [Quality Assurance â€” Verification Suite](#6-quality-assurance--verification-suite)
7. [Database Schema Overview](#7-database-schema-overview)
8. [API Surface](#8-api-surface)
9. [Compliance & Ethics](#9-compliance--ethics)
10. [Roadmap](#10-roadmap)
11. [Footer](#11-footer)

---

## 1. EXECUTIVE OVERVIEW & VALUE PROPOSITION

### What PriceCom Is

**PriceCom** is a **Real-time E-commerce Intelligence Engine** built on Django MVT. It does not merely scrape prices â€” it *fuses* heterogeneous product data from Amazon and Flipkart into a canonical, integrity-verified intelligence matrix. Every price point is timestamped, SHA-256 signed, and semantically matched to its cross-platform equivalent, creating a single source of truth for price intelligence.

The engine is designed for **zero-lag awareness**: the moment a store updates a product's price, a cascade of atomic model signals recalculates the global lowest price, re-evaluates the price velocity trend, and dispatches a user alert â€” all within the same database transaction.

### Core Value Propositions

| Pillar | Description | Business Impact |
|--------|-------------|-----------------|
| **Cross-Platform Matrix** | Semantic matching bridges Amazon Ã— Flipkart product listings using a Token-Sort + Levenshtein algorithm with >85% similarity threshold | Eliminates manual product de-duplication; builds a unified comparison layer across all registered stores |
| **Atomic Price Sync** | `StorePrice.save()` signals trigger `Product.update_lowest_price()` atomically, ensuring the dashboard always reflects the true market floor | Zero stale reads on the dashboard; eliminates the data consistency lag that plagues traditional scrapers |
| **SHA-256 Financial Integrity** | Every price record generates a cryptographic fingerprint (`price + store + timestamp`) committed to the database | Provides immutable, tamper-evident audit proof suitable for regulatory and financial compliance review |
| **Stealth Acquisition Layer** | `StealthHeaderEngine` rotates User-Agents, randomizes `Referer` headers, and uses entropy-based jitter delays on top of `robots.txt` compliance delays | Sustains long-running data collection without IP blocks or detection by enterprise anti-bot systems |
| **Predictive Intelligence Hooks** | `trend_indicator`, `search_vector`, and `metadata` JSONField schema hooks are pre-wired into the `Product` model for direct LSTM/Prophet integration | Reduces Phase 2 AI integration time from weeks to days; no schema migrations required |

---

## 2. ARCHITECTURAL DEEP-DIVE (MVT + BACKGROUND PIPELINE)

### Project Structure

```
project_root/
â”‚
â”œâ”€â”€ manage.py                       # Django entry point
â”œâ”€â”€ requirements.txt                # Pinned dependency manifest
â”œâ”€â”€ .env.example                    # Sanitized configuration template
â”œâ”€â”€ validate_env.py                 # Startup environment integrity checker
â”‚
â”œâ”€â”€ config/                         # Django project configuration
â”‚   â”œâ”€â”€ settings.py                 # Core settings (DB, Auth, Middleware)
â”‚   â”œâ”€â”€ urls.py                     # Root URL dispatcher
â”‚   â””â”€â”€ celery.py                   # Celery app instance & task routing
â”‚
â”œâ”€â”€ core/                           # Shared utilities and base classes
â”‚
â”œâ”€â”€ authentication/                 # Django-allauth & SimpleJWT integration
â”‚   â””â”€â”€ (login, register, password reset templates & views)
â”‚
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ scraper/                    # â˜… Core Intelligence Engine
â”‚   â”‚   â”œâ”€â”€ models.py               # Data models with atomic signals
â”‚   â”‚   â”œâ”€â”€ scraper_engine.py       # Platform-specific scraping logic
â”‚   â”‚   â”œâ”€â”€ stealth_engine.py       # Stealth header & jitter system
â”‚   â”‚   â”œâ”€â”€ stealth_browser.py      # Selenium stealth browser wrapper
â”‚   â”‚   â”œâ”€â”€ matcher.py              # Fuzzy Matchmaker (Semantic Engine)
â”‚   â”‚   â”œâ”€â”€ normalization.py        # Heterogeneous data normalizer
â”‚   â”‚   â”œâ”€â”€ security.py             # SSRF Shield & URL sanitizer
â”‚   â”‚   â”œâ”€â”€ security_utils.py       # Auxiliary security helpers
â”‚   â”‚   â”œâ”€â”€ signals.py              # Django signals (price cascade triggers)
â”‚   â”‚   â”œâ”€â”€ tasks.py                # Celery async task definitions
â”‚   â”‚   â”œâ”€â”€ timezone_utils.py       # UTC-aware timestamp utilities
â”‚   â”‚   â”œâ”€â”€ concurrency.py          # Thread-safety primitives
â”‚   â”‚   â”œâ”€â”€ decorators.py           # Rate-limit & security decorators
â”‚   â”‚   â”œâ”€â”€ selectors.py            # Optimized DB query selectors
â”‚   â”‚   â”œâ”€â”€ views.py                # REST + template views
â”‚   â”‚   â”œâ”€â”€ urls.py                 # App URL routes
â”‚   â”‚   â””â”€â”€ logic/                  # Domain business logic subpackage
â”‚   â”‚       â””â”€â”€ services/           # Service-layer for external integrations
â”‚   â”‚
â”‚   â”œâ”€â”€ dashboard/                  # Analytics Dashboard (Charts, Tables, Alerts)
â”‚   â”‚
â”‚   â””â”€â”€ accounts/                   # User profile, Watchlist, Wallet Bridge
â”‚
â”œâ”€â”€ ocr_uploads/                    # Tesseract OCR image upload staging directory
â”œâ”€â”€ verifytest/                     # QA Verification script suite
â””â”€â”€ static/                         # Compiled CSS, JS, image assets
```

### System Architecture â€” Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PRICECOM â€” INTELLIGENCE PIPELINE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  [USER / SCHEDULER]                                                      â”‚
â”‚        â”‚                                                                 â”‚
â”‚        â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚             [LAYER 1] ACQUISITION ENGINE                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  StealthHeaderEngine    RobotsComplianceManager   Selenium       â”‚    â”‚
â”‚  â”‚  (User-Agent Rotation)  (robots.txt + jitter)     (JS-rendered) â”‚    â”‚
â”‚  â”‚  SecurityShield â†’ SSRF Check â†’ Sanitized URL                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚                                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           [LAYER 2] NORMALIZATION & MATCHING ENGINE              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  normalization.py                matcher.py                      â”‚    â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    â”‚
â”‚  â”‚  Heterogeneous Data Normalizer   ProductSimilarityEngine          â”‚    â”‚
â”‚  â”‚  Raw HTML â†’ UnifiedProduct       Token-Sort + Levenshtein        â”‚    â”‚
â”‚  â”‚  Schema (price, title, URL)      >85% â†’ Same Physical Product    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚                                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           [LAYER 3] ATOMIC PERSISTENCE LAYER                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  StorePrice.save()                                               â”‚    â”‚
â”‚  â”‚    â†’ SHA-256 hash generated (price + store + UTC timestamp)      â”‚    â”‚
â”‚  â”‚    â†’ Committed to MySQL                                          â”‚    â”‚
â”‚  â”‚    â†’ Signal fires â†’ Product.update_lowest_price()               â”‚    â”‚
â”‚  â”‚        â†’ Atomic field update: current_lowest_price               â”‚    â”‚
â”‚  â”‚        â†’ get_price_velocity() â†’ trend_indicator updated         â”‚    â”‚
â”‚  â”‚  PriceHistory record written (immutable audit trail)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚                                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           [LAYER 4] ALERTING & DASHBOARD                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  PriceAlert evaluation â†’ NotificationLog (Audit Trail)           â”‚    â”‚
â”‚  â”‚  Watchlist.sync_with_wallet() â†’ Reward Packet (SHA-256 signed)  â”‚    â”‚
â”‚  â”‚  Dashboard: Freshness Heartbeat â†’ status-live / delayed / stale  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Celery / Redis Background Pipeline

The acquisition layer is fully **asynchronous**. Scraping tasks are dispatched to a Redis-backed Celery queue, ensuring the Django web process never blocks on I/O-bound scraping operations.

```
[Django View / Management Command]
        â”‚
        â”‚ .delay() / .apply_async()
        â–¼
[Redis Message Broker] â”€â”€â”€â”€â”€â”€â”€â”€â–º [Celery Worker Process]
                                          â”‚
                                    tasks.py:
                                    scrape_product_task()
                                    bulk_refresh_task()
                                    ocr_process_image_task()
                                          â”‚
                                    Model.save() â†’ Atomic Signal â†’ DB
```

**Tasks defined in `tasks.py`:**

| Task Name | Trigger | Function |
|-----------|---------|----------|
| `scrape_product_task` | On-demand / Scheduled | Scrapes a single product URL, normalizes data, persists to `StorePrice` |
| `bulk_refresh_task` | Cron / Beat scheduler | Iterates all active `StorePrice` records and re-scrapes in parallel |
| `ocr_process_image_task` | Upload event | Passes `ProductImage` through Tesseract, extracts price text, updates record |
| `notify_price_drop_task` | Price change signal | Evaluates `PriceAlert` targets, dispatches email via SMTP |

---

## 3. MODULE DEEP-DIVE â€” THE INTELLIGENCE LAYER

---

### 3.1 Fuzzy Matchmaker â€” Semantic Product Engine

**File:** `apps/scraper/matcher.py`  
**Class:** `ProductSimilarityEngine`

The Fuzzy Matchmaker solves the core problem of **Heterogeneous Data Normalization**: Amazon and Flipkart list the same physical product under slightly different names, brand formats, and bracket suffixes. A naive string comparison will fail. PriceCom's matching engine uses a **two-phase algorithm** to guarantee >85% precision:

#### Phase A â€” Preprocessing (Noise Removal)

```python
@staticmethod
def preprocess_title(text: str) -> str:
    """
    Phase A: Preprocessing (Noise Removal).
    Case-Folding, stop-word stripping, and bracket/suffix removal via Regex.
    Ensures 'Apple iPhone 14 (128GB, Black)' and 'iphone 14 128gb black'
    collapse to the same canonical token set.
    """
    text = text.lower()
    text = re.sub(r'\(.*?\)|\[.*?\]', '', text)   # Remove bracket noise
    stop_words = ['apple', 'samsung', 'brand', 'new', 'unlocked', 'the', 'and', 'a', 'for']
    tokens = text.split()
    return " ".join([t for t in tokens if t not in stop_words and len(t) > 1])
```

#### Phase B â€” Token-Sort + Levenshtein Scoring

```python
@staticmethod
def get_similarity_score(title_a: str, title_b: str) -> float:
    """
    Phase B: Token-Sort + Fuzzy Matching.
    Alphabetically sorts word tokens before computing Levenshtein ratio.
    Handles word-order variations: 'Black 128GB iPhone' == 'iPhone 128GB Black'.
    """
    clean_a = ProductSimilarityEngine.preprocess_title(title_a)
    clean_b = ProductSimilarityEngine.preprocess_title(title_b)
    sorted_a = " ".join(sorted(clean_a.split()))
    sorted_b = " ".join(sorted(clean_b.split()))
    return SequenceMatcher(None, sorted_a, sorted_b).ratio()
```

#### Phase C â€” Matrix Grouping

```python
def match_products_across_stores(product_list: List[dict]) -> List[List[dict]]:
    """
    Semantic Product Grouping: Clusters chaotic store items into a unified
    Comparison Matrix. If similarity > 0.85, items are guaranteed to be the
    same physical product and are placed side-by-side in the intelligence grid.
    """
```

**Match Decision:**

| Similarity Score | Decision | Action |
|-----------------|----------|--------|
| â‰¥ 0.85 | **MATCH** â€” Same physical product | Merged into a shared matrix row |
| < 0.85 | **NO MATCH** â€” Different product | New row instantiated in the matrix |

---

### 3.2 Atomic Price Sync â€” Self-Updating Data Model

**File:** `apps/scraper/models.py` â€” `StorePrice.save()` â†’ `Product.update_lowest_price()`

This is the core **self-performing architecture** of PriceCom. No cron job or management command is required to keep `Product.current_lowest_price` accurate. The moment any `StorePrice` record is saved â€” whether by a Celery task, a manual admin action, or an API call â€” **a cascade triggers automatically**:

```python
# In StorePrice.save()
def save(self, *args, **kwargs) -> None:
    # 1. Generate SHA-256 integrity fingerprint BEFORE committing
    now_str = timezone.now().isoformat()
    raw_string = f"{self.current_price}-{self.store_name}-{now_str}"
    self.price_hash = hashlib.sha256(raw_string.encode('utf-8')).hexdigest()
    super().save(*args, **kwargs)

    # 2. Atomic Cascade: Force parent Product recalculation immediately after save
    if self.product:
        self.product.update_lowest_price()   # â† Self-performing signal

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# In Product.update_lowest_price()
def update_lowest_price(self) -> None:
    """
    Atomic Price Calculation: Queries all active StorePrice records,
    computes the minimum, updates trend_indicator, and atomically commits
    only the changed fields â€” avoiding a full-model save and
    preventing race conditions under concurrent scraper execution.
    """
    active_prices = self.prices.filter(is_available=True).values_list('current_price', flat=True)
    if active_prices:
        self.current_lowest_price = min(active_prices)
        self.get_price_velocity()   # Update STABLE / DROPPING / VOLATILE
        self.save(update_fields=['current_lowest_price', 'trend_indicator'])
        #         â†‘ Partial save: Only two fields written â†’ Minimal lock time
```

**The cascade sequence visualized:**

```
StorePrice(price=899).save()
        â”‚
        â”œâ”€â”€ [1] SHA-256 fingerprint generated & stored
        â”œâ”€â”€ [2] super().save() â†’ MySQL commit
        â””â”€â”€ [3] product.update_lowest_price()
                    â”‚
                    â”œâ”€â”€ [4] Query: min(active StorePrice)
                    â”œâ”€â”€ [5] Product.current_lowest_price = 899
                    â”œâ”€â”€ [6] get_price_velocity() â†’ "DROPPING" (if < 70% of MRP)
                    â””â”€â”€ [7] Product.save(update_fields=[...]) â†’ atomic partial write
```

---

### 3.3 Freshness Heartbeat â€” UTC-Aware Data Pulse

**File:** `apps/scraper/models.py` â€” `Product.get_freshness_status()`  
**Support:** `apps/scraper/timezone_utils.py`

Every product carries a **Freshness Status** â€” a real-time signal derived from `updated_at` (UTC-aware) that the Dashboard UI translates into a live CSS class:

```python
def get_freshness_status(self) -> str:
    """
    Global Display Logic: Translates data age (UTC-aware diff)
    into Dashboard Pulse UI classes for real-time freshness signaling.
    """
    diff = timezone.now() - self.updated_at   # UTC-aware subtraction

    if diff < timedelta(hours=1):
        return 'status-live'      # ğŸŸ¢ Green pulse  â€” scrape is hot
    elif diff < timedelta(hours=24):
        return 'status-delayed'   # ğŸŸ¡ Yellow pulse â€” data is aging
    else:
        return 'status-stale'     # ğŸ”´ Red pulse    â€” scrape needed
```

| Status Class | Threshold | Dashboard Indicator |
|--------------|-----------|---------------------|
| `status-live` | Updated < 1 hour ago | ğŸŸ¢ Live green pulse dot |
| `status-delayed` | Updated 1â€“24 hours ago | ğŸŸ¡ Amber warning dot |
| `status-stale` | Updated > 24 hours ago | ğŸ”´ Red stale dot |

All timestamps are enforced as **UTC-aware** via `pytz` during `Product.save()`, preventing the timezone-naive comparison bugs that cause silent data staleness in naive datetime implementations.

---

### 3.4 Predictive Price Intelligence â€” LSTM/Prophet Ready

**File:** `apps/scraper/models.py` â€” `Product.get_price_velocity()`

The `Product` model is **pre-wired** for AI/ML integration. Three schema hooks are already in place, requiring zero migration work when Phase 2 models are integrated:

| Field | Type | Current Role | Phase 2 AI Role |
|-------|------|-------------|-----------------|
| `trend_indicator` | `CharField` | `STABLE` / `DROPPING` / `VOLATILE` rule-based | LSTM output label (predicted direction) |
| `search_vector` | `TextField` | NLP pre-processing placeholder | Word2Vec / SBERT embedding storage |
| `metadata` | `JSONField` | Extensible dict for custom signals | Anomaly detection flags, Prophet forecast deltas |

**Current velocity logic (functional mock for dashboard):**

```python
def get_price_velocity(self) -> str:
    """
    Phase 1: Rule-based momentum scoring.
    Phase 2 AI Hook: Output fed into LSTM time-series model.
    ratio < 0.70 â†’ DROPPING  (>30% below MRP â€” significant deal signal)
    ratio > 1.10 â†’ VOLATILE  (price surge â€” demand spike indicator)
    else         â†’ STABLE
    """
    ratio = self.current_lowest_price / self.base_price
    if ratio < Decimal('0.7'):
        return "DROPPING"
    elif ratio > Decimal('1.1'):
        return "VOLATILE"
    return "STABLE"
```

**Price History** (`PriceHistory` model) records every change with `change_percentage`, `trend`, `is_significant_drop`, and a custom `PriceHistoryManager.get_biggest_drops()` query â€” giving an LSTM model a clean, indexed time-series sequence per product ready for training.

---

### 3.5 Wallet & Reward Bridge â€” Loyalty Extrapolation

**File:** `apps/scraper/models.py` â€” `Watchlist.sync_with_wallet()` & `Product.calculate_purchase_reward()`

PriceCom includes a **Wallet & Reward Bridge** that extrapolates loyalty reward points directly from the live lowest price, and cryptographically signs each reward transaction packet to prevent tampering.

```python
# Step 1: Reward Calculation (1% of definitive lowest price)
def calculate_purchase_reward(self) -> Decimal:
    """
    Wallet Bridge Logic: Mathematically extrapolates 1% Platform Wallet
    credit from the verified, atomically-calculated lowest price.
    """
    reward = self.current_lowest_price * Decimal('0.01')
    return round(reward, 2)

# Step 2: Secure Transaction Packet Generation
def sync_with_wallet(self, user_wallet_id: str) -> dict:
    """
    Cross-App Dependency Management: Prepares a SHA-256 signed transaction
    packet bridging Watchlist Price Drop to the Wallet Ledger.

    Non-Repudiation: The auth_signature field is a SHA-256 hash of:
      (wallet_id + watchlist_uuid + reward_amount)
    This ensures no reward claim can be altered in transit without
    invalidating the cryptographic signature â€” providing non-repudiable
    proof of the original reward intent.
    """
    calculated_reward = self.product.calculate_purchase_reward()
    payload_str = f"{user_wallet_id}-{self.uuid}-{calculated_reward}"
    signature = hashlib.sha256(payload_str.encode('utf-8')).hexdigest()

    return {
        "status": "READY",
        "wallet_id": user_wallet_id,
        "reward_amount": str(calculated_reward),
        "watchlist_ref": str(self.uuid),
        "auth_signature": signature   # â† Non-repudiable reward proof
    }
```

**Reward lifecycle:**

```
Product.current_lowest_price (â‚¹899)
        â”‚
        â””â”€â”€ calculate_purchase_reward() â†’ â‚¹8.99
                â”‚
                â””â”€â”€ Watchlist.sync_with_wallet(user_wallet_id)
                        â”‚
                        â”œâ”€â”€ SHA-256(wallet_id + uuid + 8.99) â†’ auth_signature
                        â””â”€â”€ Signed packet dispatched to Wallet Ledger
```

---

### 3.6 Audit Trail â€” Non-Repudiable Notification Engine

**File:** `apps/scraper/models.py` â€” `NotificationLog`  
**Verbose Name:** `"Audit Trail"` (defined in `Meta`)

Every notification attempt â€” whether successful, failed, or suppressed â€” is permanently logged in the `NotificationLog` table. This constitutes a **non-repudiable audit trail**: no event can be un-sent, and no delivery failure can be silently discarded.

| Field | Purpose |
|-------|---------|
| `uuid` | Globally unique traceability ID for every notification event |
| `user` | The recipient â€” immutable FK, preserved even on account deactivation |
| `product` | The triggering product â€” nullable for system-level alerts |
| `price_at_alert` | Snapshot of the price **at the moment the alert fired** |
| `status` | `PENDING â†’ SENT / FAILED / SUPPRESSED` state machine |
| `alert_type` | `Drop / Restock / System` â€” classifies the event type |
| `intent_timestamp` | UTC timestamp of the notification intent (auto-set, immutable) |
| `smtp_response_code` | Raw SMTP server response code for delivery trace |
| `error_message` | Full error payload (truncated at 2000 chars) for failure diagnostics |

**Non-Repudiation guarantee:** The `intent_timestamp` field uses `auto_now_add=True` â€” it is set exactly **once**, atomically, at record creation, and can never be updated. Combined with the `uuid`, every notification event has an immutable, globally unique identity that can be used as evidence in a compliance review.

---

### 3.7 OCR Price Extraction â€” Tesseract Vision Engine

**File:** `apps/scraper/models.py` â€” `ProductImage`  
**Upload Path:** `ocr_uploads/%Y/%m/%d/`  
**Celery Task:** `ocr_process_image_task`

PriceCom supports **image-to-price conversion** via the Tesseract OCR engine. A user can upload a screenshot of a product listing, and the background pipeline will automatically extract the price text and associate it with a product record.

**Processing States:**

```
[Upload] â†’ PENDING â†’ [Celery Worker picks up] â†’ PROCESSING
                                                      â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚                            â”‚
                                   COMPLETED                   LOW_CONFIDENCE
                               (text extracted)            (OCR confidence < threshold)
                                        â”‚
                                   FAILED (exception)
```

| State | Meaning |
|-------|---------|
| `PENDING` | Image uploaded, awaiting Celery task pickup |
| `PROCESSING` | Tesseract actively processing |
| `COMPLETED` | Price text successfully extracted and stored in `extracted_text` |
| `LOW_CONFIDENCE` | Tesseract confidence score below threshold â€” manual review required |
| `FAILED` | Processing exception; error logged |

---

## 4. CYBERSECURITY & DATA INTEGRITY â€” THE SHIELD

---

### 4.1 SSRF Shield â€” URL Sanitization

**File:** `apps/scraper/security.py` â€” `SecurityShield.sanitize_product_url()`

**Server-Side Request Forgery (SSRF)** is the primary attack vector against scraping pipelines: a malicious user submits an internal URL (e.g., `http://169.254.169.254/`) and tricks the server into fetching it. PriceCom's `SecurityShield` prevents this with a **strict domain whitelist**:

```python
class SecurityShield:
    """
    Topper Grade Cybersecurity Shield Layer.
    Protects against malformed URLs, SSRF, and XSS injection via URL fields.
    """

    # Only these domains may ever be fetched â€” all others are hard-blocked
    ALLOWED_DOMAINS = [
        'www.amazon.in',
        'www.amazon.com',
        'www.flipkart.com',
        'flipkart.com',
    ]

    @staticmethod
    def sanitize_product_url(url: str) -> Optional[str]:
        parsed = urlparse(url)

        # SSRF Protection: Any domain not in whitelist is hard-rejected
        if parsed.netloc not in SecurityShield.ALLOWED_DOMAINS:
            logger.warning(f"SSRF Prevention: Blocked domain {parsed.netloc}")
            return None   # â† Returns None, not an exception â€” safe for callers

        # Strip all tracking parameters (UTMs, session IDs)
        # Exception: Flipkart requires 'pid' parameter for product resolution
        clean_query = ""
        if 'flipkart' in parsed.netloc:
            match = re.search(r'pid=([A-Z0-9]+)', parsed.query)
            if match:
                clean_query = f"pid={match.group(1)}"

        return urlunparse(parsed._replace(query=clean_query, fragment=''))
```

**Protection Matrix:**

| Attack Vector | Shield Response |
|---------------|----------------|
| Internal IP (`192.168.x.x`, `169.254.x.x`) | Hard-blocked by domain whitelist |
| `file://` protocol | Blocked â€” not in `ALLOWED_DOMAINS` |
| Redirect chains to internal hosts | Blocked at the request level by domain enforcement |
| UTM / Session tracking parameters | Stripped from URL before storage |
| XSS via URL fragment (`#<script>`) | Stripped via `fragment=''` replacement |

---

### 4.2 SHA-256 Financial Integrity Hashing

**File:** `apps/scraper/models.py` â€” `StorePrice.save()` & `PriceHistory.integrity_hash`

Every financial data point in PriceCom is protected by a **SHA-256 cryptographic fingerprint**, generated at write time and stored alongside the record. This serves as the data integrity backbone of the audit system.

```python
# StorePrice integrity fingerprint
raw_string = f"{self.current_price}-{self.store_name}-{now_str}"
self.price_hash = hashlib.sha256(raw_string.encode('utf-8')).hexdigest()
# â†‘ Encodes: price value + source store + exact UTC ISO timestamp
# â†’ Any post-hoc tampering of price or timestamp invalidates the hash
```

**Integrity Model:**

```
At write time:
  raw       = f"{price}-{store}-{utc_timestamp}"
  price_hash = SHA-256(raw) â†’ 64-char hex string stored in DB

At verification time:
  recompute = SHA-256(f"{stored_price}-{stored_store}-{stored_timestamp}")
  assert recompute == stored price_hash
  â†’ PASS: Record is untampered
  â†’ FAIL: Record has been altered â€” forensic alert triggered
```

Both `StorePrice.price_hash` and `PriceHistory.integrity_hash` carry independent fingerprints, allowing the verifier to cross-reference the current price state against its historical lineage â€” providing a chain-of-custody guarantee for every price point.

---

### 4.3 Stealth Scraping Engine

**File:** `apps/scraper/stealth_engine.py`

The acquisition layer is built around three cooperating classes that together constitute the **Stealth Scraping Engine**:

#### `StealthHeaderEngine` â€” Identity Vault

Maintains a curated **Identity Vault** of modern browser User-Agent strings (Chrome 120/121, Safari 17, Firefox 122) and generates a complete, **browser-consistent header set** per request â€” including `sec-ch-ua`, `Sec-Fetch-*`, and rotating `Referer` headers â€” making each request statistically indistinguishable from a real browser session.

```
User-Agent Pool:
  Chrome 120 (Windows 11)  â†â”€â”
  Chrome 121 (Windows 11)     â”‚ Random pick per session
  Safari 17 (macOS)          â”‚ Sticky within session
  Firefox 122 (Windows)   â†â”€â”€â”˜

Session Sticky Logic: once an identity is chosen for a session,
all subsequent requests in that session use the same UA to avoid
inconsistent fingerprints across request chains.
```

#### `HumanBehavior` â€” Temporal Entropy Engine

```python
@staticmethod
def human_like_delay(min_wait: float = 2.0, max_wait: float = 7.0):
    """
    Randomized Jitter Engine.
    Creates 'Temporal Entropy' to break the mathematical signature of a bot.
    A bot sends requests at fixed intervals; a human does not.
    random.uniform(2.0, 7.0) ensures no two requests share a timing pattern.
    """
    delay = random.uniform(min_wait, max_wait)
    time.sleep(delay)
```

#### `RobotsComplianceManager` â€” Ethical Crawl Guardian

Fetches and caches each domain's `robots.txt`, extracts the `Crawl-delay` directive, and applies it as the **minimum** delay before any request. The jitter from `HumanBehavior` is then added **on top** of this required delay, ensuring the engine is simultaneously compliant and stealthy.

#### `AdvancedScraperSession` â€” Integrated Resilience Unit

```
Per-request flow:
  1. RobotsComplianceManager â†’ required_delay (e.g., 2.0s)
  2. HumanBehavior.jitter()  â†’ +0.5 to +2.0s entropy
  3. StealthHeaderEngine     â†’ full browser header set applied
  4. Exponential Backoff Loop (max 3 retries):
       429 / 503 received â†’ Retry-After header respected
                          â†’ Identity rotated ("New User" simulation)
                          â†’ Backoff: 5s â†’ 10s â†’ 20s
```

---

## 5. DEPLOYMENT & OPERATION GUIDE

### 5.1 Prerequisites

| Dependency | Version | Purpose |
|------------|---------|---------|
| Python | 3.10+ | Runtime |
| MySQL / MariaDB | 8.0+ | Primary database |
| Redis | 7.0+ | Celery message broker |
| Google Chrome | Latest | Selenium scraping |
| Tesseract OCR | 5.0+ | Image-to-price extraction (`ocr_uploads`) |
| Node.js | 18+ | Optional â€” for static asset compilation |

---

### 5.2 Installation

```bash
# â”€â”€ 1. Clone the repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
git clone https://github.com/your-org/pricecom.git
cd pricecom/project_root

# â”€â”€ 2. Create and activate virtual environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m venv .venv
source .venv/bin/activate          # Linux / macOS
# .venv\Scripts\activate           # Windows PowerShell

# â”€â”€ 3. Install Python dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pip install --upgrade pip
pip install -r requirements.txt

# â”€â”€ 4. Configure environment variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cp .env.example .env
# Edit .env â€” fill in DB credentials, Django secret key, email config

# â”€â”€ 5. Validate environment before first boot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python validate_env.py
# âœ” All required environment variables present
# âœ” Database connection successful
# âœ” Redis connection successful

# â”€â”€ 6. Apply database migrations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python manage.py migrate

# â”€â”€ 7. Create superuser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python manage.py createsuperuser

# â”€â”€ 8. Collect static files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python manage.py collectstatic --noinput

# â”€â”€ 9. Start Django development server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python manage.py runserver
#   â†’ http://127.0.0.1:8000/
```

---

### 5.3 Redis & Celery Background Pipeline

```bash
# Terminal 1 â€” Start Redis (must be running before Celery)
redis-server
# Redis ready on port 6379

# Terminal 2 â€” Start Celery worker (processes scraping tasks)
celery -A config worker --loglevel=info --concurrency=4
# [celery@host] ready.

# Terminal 3 (Optional) â€” Start Celery Beat (periodic task scheduler)
celery -A config beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
# Periodic tasks: bulk_refresh_task every 6 hours

# Terminal 4 â€” Django server
python manage.py runserver
```

**Trigger a scrape manually:**

```bash
# From Django shell â€” dispatch a scrape task to Celery queue
python manage.py shell
>>> from apps.scraper.tasks import scrape_product_task
>>> scrape_product_task.delay("https://www.amazon.in/dp/B0EXAMPLE")
# <AsyncResult: uuid-xxxx-xxxx>  â† Task queued in Redis
```

---

### 5.4 Environment Configuration

```env
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PRICECOM â€” ENVIRONMENT CONFIGURATION TEMPLATE
#  Copy to .env and fill in production values.
#  NEVER commit .env to version control.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ Django Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DJANGO_SECRET_KEY=your-cryptographically-strong-secret-key-here
DJANGO_DEBUG=False                     # Set True only in development
DJANGO_ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com

# â”€â”€ Database (MySQL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_NAME=pricecom_production
DB_USER=pricecom_user
DB_PASSWORD=your_strong_db_password
DB_HOST=localhost
DB_PORT=3306

# â”€â”€ Celery / Redis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/1

# â”€â”€ Email / SMTP (Notification Engine) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMAIL_BACKEND=django.core.mail.backends.smtp.EmailBackend
EMAIL_HOST=smtp.sendgrid.net
EMAIL_PORT=587
EMAIL_USE_TLS=True
EMAIL_HOST_USER=apikey
EMAIL_HOST_PASSWORD=your_sendgrid_api_key

# â”€â”€ Scraper Behaviour â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRAPER_MIN_DELAY_SECONDS=2.0
SCRAPER_MAX_DELAY_SECONDS=7.0
SCRAPER_MAX_RETRIES=3
```

---

## 6. QUALITY ASSURANCE â€” VERIFICATION SUITE

PriceCom ships with a dedicated QA verification suite in the `verifytest/` directory. These scripts constitute the **professional mark** of a production-ready system â€” a structured, repeatable verification protocol that can be run before any deployment.

```bash
# â”€â”€ Full System Integration Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python verifytest/verify_full_system.py
# Verifies:
#   âœ” Django project loads without import errors
#   âœ” All database models migrate cleanly
#   âœ” StorePrice.save() â†’ Product.update_lowest_price() cascade fires correctly
#   âœ” PriceHistory record created on each StorePrice save
#   âœ” Celery task queue reachable (Redis ping)
#   âœ” NotificationLog Audit Trail writing correctly

# â”€â”€ Security Verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python verifytest/verify_security.py
# Verifies:
#   âœ” SSRF: Blocked domains return None (not an exception)
#   âœ” SSRF: Allowed domains (amazon.in, flipkart.com) pass sanitization
#   âœ” SHA-256: price_hash generated on every StorePrice save
#   âœ” SHA-256: hash changes when price changes (tamper detection works)
#   âœ” URL stripping: UTM / session parameters removed correctly
#   âœ” Flipkart pid parameter preserved through sanitization

# â”€â”€ Pipeline Test (manual scrape + DB write) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python test_pipeline.py

# â”€â”€ Environment Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python validate_env.py
```

| Script | Category | What It Proves |
|--------|----------|----------------|
| `verify_full_system.py` | Integration | End-to-end pipeline integrity |
| `verify_security.py` | Security | SSRF, SHA-256, URL sanitization correctness |
| `test_pipeline.py` | Smoke Test | Live scrape write-through to database |
| `validate_env.py` | Environment | All configuration variables present and valid |

---

## 7. DATABASE SCHEMA OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Category   â”‚â”€â”€â”€â”€â”€â”€>â”‚     Product       â”‚<â”€â”€â”€â”€â”€â”€â”‚    StorePrice    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  FK   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  FK   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ name         â”‚       â”‚ uuid (UUID)       â”‚       â”‚ store_name       â”‚
â”‚ slug         â”‚       â”‚ name              â”‚       â”‚ current_price    â”‚
â”‚ icon         â”‚       â”‚ slug              â”‚       â”‚ product_url      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ sku               â”‚       â”‚ is_available     â”‚
                       â”‚ brand_name        â”‚       â”‚ price_hash â˜…     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ base_price        â”‚       â”‚ last_updated     â”‚
â”‚    Tag       â”‚<â”€â”€â”€â”€â”€>â”‚ current_lowest_   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  M2M  â”‚   price           â”‚                â”‚ FK
â”‚ name         â”‚       â”‚ trend_indicator â˜… â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ slug         â”‚       â”‚ search_vector â˜…   â”‚       â”‚   PriceHistory   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ metadata â˜…        â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                       â”‚ is_active         â”‚       â”‚ price            â”‚
                       â”‚ is_featured       â”‚       â”‚ change_percentageâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ trend            â”‚
                                 â”‚ FK               â”‚ is_significant_  â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   drop           â”‚
                       â”‚    Watchlist      â”‚       â”‚ integrity_hash â˜… â”‚
                       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ target_price      â”‚
                       â”‚ reward_points_    â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   eligible        â”‚       â”‚   PriceAlert     â”‚
                       â”‚ potential_reward  â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                       â”‚ is_reward_claimed â”‚       â”‚ product_url      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ target_price     â”‚
                                                   â”‚ alert_priority   â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ is_triggered     â”‚
                       â”‚ NotificationLog   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
                       â”‚ uuid (trace ID)   â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ price_at_alert    â”‚       â”‚  ProductImage    â”‚
                       â”‚ status            â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                       â”‚ alert_type        â”‚       â”‚ image (OCR)      â”‚
                       â”‚ intent_timestamp â˜…â”‚       â”‚ extracted_text   â”‚
                       â”‚ smtp_response_codeâ”‚       â”‚ status           â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â˜… = Intelligence / Security field
```

---

## 8. API SURFACE

PriceCom exposes a **Django REST Framework** API secured with **SimpleJWT** bearer tokens.

| Method | Endpoint | Auth | Description |
|--------|----------|------|-------------|
| `POST` | `/api/auth/token/` | Public | Obtain JWT access + refresh tokens |
| `POST` | `/api/auth/token/refresh/` | Public | Refresh expired access token |
| `GET` | `/api/products/` | JWT | List all active products with current_lowest_price |
| `GET` | `/api/products/{uuid}/` | JWT | Product detail with full price history |
| `POST` | `/api/scraper/trigger/` | JWT + Staff | Trigger async scrape task for a URL |
| `GET` | `/api/watchlist/` | JWT | Current user's watchlist |
| `POST` | `/api/watchlist/` | JWT | Add product to watchlist with target_price |
| `DELETE` | `/api/watchlist/{uuid}/` | JWT | Remove from watchlist |
| `POST` | `/api/ocr/upload/` | JWT | Upload product image for Tesseract processing |
| `GET` | `/api/audit/` | JWT + Staff | Full NotificationLog audit trail |

**Authentication header format:**

```
Authorization: Bearer <access_token>
```

---

## 9. COMPLIANCE & ETHICS

| Principle | Implementation |
|-----------|----------------|
| **robots.txt Compliance** | `RobotsComplianceManager` fetches and respects `Crawl-delay` for every domain before any request is made |
| **Rate Limiting** | `django-ratelimit` decorators applied to all user-facing scrape-trigger endpoints |
| **Data Minimization** | Only price, URL, availability, and product title are stored â€” no personal data is scraped from product pages |
| **CORS Policy** | `django-cors-headers` configured with strict origin whitelist |
| **JWT Expiry** | Access tokens expire in 15 minutes; refresh tokens in 7 days â€” limiting exposure window |
| **SSRF Prevention** | All user-submitted URLs hard-blocked against non-whitelisted domains |
| **No Credential Storage** | Scraper engine stores no session cookies or authentication state between runs |

---

## 10. ROADMAP

| Version | Feature | Status |
|---------|---------|--------|
| **v1.0** | Core scraper, Atomic Price Sync, Security Shield, Dashboard | âœ… Shipped |
| **v1.1** | Celery + Redis full async pipeline, Beat scheduler | âœ… Shipped |
| **v1.2** | Tesseract OCR image-to-price pipeline | âœ… Shipped |
| **v1.3** | Full REST API with SimpleJWT | âœ… Shipped |
| **v2.0** | LSTM price prediction model (Facebook Prophet integration) | ğŸ”² Planned Q3 2026 |
| **v2.1** | SBERT semantic search vector integration | ğŸ”² Planned Q3 2026 |
| **v2.2** | Mobile price alert push notifications (Firebase FCM) | ğŸ”² Planned Q4 2026 |
| **v2.3** | Distributed scraping with Celery chord (parallel store workers) | ğŸ”² Planned Q4 2026 |
| **v3.0** | Multi-tenant SaaS mode with per-user scrape quota | ğŸ”² Vision 2027 |

---

## 11. FOOTER

---

<div align="center">

### PRICECOM â€” REAL-TIME E-COMMERCE INTELLIGENCE ENGINE

---

```
Classification:     Open-Source / Academic Showcase
Architecture:       Django 4.2 MVT Â· Celery Â· Redis Â· MySQL Â· Selenium Â· Tesseract OCR
Security Standard:  SSRF Shield Â· SHA-256 Integrity Â· JWT Authentication Â· Rate Limiting
Documentation:      Antigravity Grade â€” IBM Enterprise Review Ready
Version:            1.3.0
Last Updated:       February 2026
```

| Role | Contact |
|------|---------|
| Project Lead & Architect | Priyanshu â€” GitHub: `Priyanshu10s` |

---

> *"A price is just a number. An intelligence engine turns that number into a decision."*
>
> â€” PriceCom Architecture Team

---

**Â© 2026 PriceCom. Built with Django, secured with SHA-256, powered by intelligence.**

</div>
